{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ef62c0",
   "metadata": {},
   "source": [
    "# EU Regional Economic Impact Model (2022)\n",
    "This notebook builds the backend of a scalable economic impact model for the EU, using Eurostat's 2022 FIGARO supply and use tables and regional data. It closely follows the RHOMOLO V4 methodology developed by the Joint Research Centre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe137e2e",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Define File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd3109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Define local data directories\n",
    "data_dir = \"C:/Users/dzsve/Dezernat Zukunft e.V/DZ-Schalte - Dokumente/Sven/IO Model/Data/2022\"\n",
    "previous_data_dir = \"C:/Users/dzsve/Dezernat Zukunft e.V/DZ-Schalte - Dokumente/Sven/IO Model/Data/Previous years\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3bfcc",
   "metadata": {},
   "source": [
    "## 2. Supply and Use Tables (FIGARO)\n",
    "\n",
    "### 2.1 Load and Inspect SUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad6e627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AR_A01</th>\n",
       "      <th>AR_A02</th>\n",
       "      <th>AR_A03</th>\n",
       "      <th>AR_B</th>\n",
       "      <th>AR_C10T12</th>\n",
       "      <th>AR_C13T15</th>\n",
       "      <th>AR_C16</th>\n",
       "      <th>AR_C17</th>\n",
       "      <th>AR_C18</th>\n",
       "      <th>AR_C19</th>\n",
       "      <th>...</th>\n",
       "      <th>ZA_P85</th>\n",
       "      <th>ZA_Q86</th>\n",
       "      <th>ZA_Q87_88</th>\n",
       "      <th>ZA_R90T92</th>\n",
       "      <th>ZA_R93</th>\n",
       "      <th>ZA_S94</th>\n",
       "      <th>ZA_S95</th>\n",
       "      <th>ZA_S96</th>\n",
       "      <th>ZA_T</th>\n",
       "      <th>ZA_U</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rowLabels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR_CPA_A01</th>\n",
       "      <td>73118.056</td>\n",
       "      <td>59.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4014.862</td>\n",
       "      <td>62.723</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR_CPA_A02</th>\n",
       "      <td>0.000</td>\n",
       "      <td>921.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR_CPA_A03</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2695.841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>403.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR_CPA_B</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31549.851</td>\n",
       "      <td>39.558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR_CPA_C10T12</th>\n",
       "      <td>1713.321</td>\n",
       "      <td>0.000</td>\n",
       "      <td>104.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105373.108</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2944 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AR_A01   AR_A02    AR_A03       AR_B   AR_C10T12  AR_C13T15  \\\n",
       "rowLabels                                                                       \n",
       "AR_CPA_A01     73118.056   59.295     0.000      0.000    4014.862     62.723   \n",
       "AR_CPA_A02         0.000  921.066     0.000      0.000       2.048      0.000   \n",
       "AR_CPA_A03         0.000    0.000  2695.841      0.000     403.270      0.000   \n",
       "AR_CPA_B           0.000    0.000     0.000  31549.851      39.558      0.000   \n",
       "AR_CPA_C10T12   1713.321    0.000   104.449      0.000  105373.108      0.702   \n",
       "\n",
       "               AR_C16  AR_C17  AR_C18  AR_C19  ...  ZA_P85  ZA_Q86  ZA_Q87_88  \\\n",
       "rowLabels                                      ...                              \n",
       "AR_CPA_A01      0.000     0.0     0.0   0.000  ...     0.0     0.0        0.0   \n",
       "AR_CPA_A02     20.145     0.0     0.0   0.000  ...     0.0     0.0        0.0   \n",
       "AR_CPA_A03      0.000     0.0     0.0   0.000  ...     0.0     0.0        0.0   \n",
       "AR_CPA_B        0.000     0.0     0.0   0.000  ...     0.0     0.0        0.0   \n",
       "AR_CPA_C10T12   0.000     0.0     0.0   0.698  ...     0.0     0.0        0.0   \n",
       "\n",
       "               ZA_R90T92  ZA_R93  ZA_S94  ZA_S95  ZA_S96  ZA_T  ZA_U  \n",
       "rowLabels                                                             \n",
       "AR_CPA_A01           0.0     0.0     0.0     0.0     0.0   0.0     0  \n",
       "AR_CPA_A02           0.0     0.0     0.0     0.0     0.0   0.0     0  \n",
       "AR_CPA_A03           0.0     0.0     0.0     0.0     0.0   0.0     0  \n",
       "AR_CPA_B             0.0     0.0     0.0     0.0     0.0   0.0     0  \n",
       "AR_CPA_C10T12        0.0     0.0     0.0     0.0     0.0   0.0     0  \n",
       "\n",
       "[5 rows x 2944 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supply_table = pd.read_csv(os.path.join(data_dir, \"FIGARO_supply_2022.csv\"), index_col=0)\n",
    "use_table = pd.read_csv(os.path.join(data_dir, \"FIGARO_use_2022.csv\"), index_col=0)\n",
    "\n",
    "supply_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36598a70",
   "metadata": {},
   "source": [
    "### 2.2 Correct Negative Values in SUTs\n",
    "\n",
    "Following the RHOMOLO method, theoretically invalid negative values are replaced with zero and correction factors (stock adjustments) are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da87a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct negative values in supply table\n",
    "if(supply_table < 0).any().any():\n",
    "    original_supply_table = supply_table.copy()\n",
    "    supply_corrections = supply_table.copy()\n",
    "    supply_table[supply_table < 0] = 0 #replace negatives with 0\n",
    "    supply_corrections = (supply_corrections - supply_table) #store corrections\n",
    "    \n",
    "    # Add discrepancy row & column in the supply table\n",
    "    supply_table.loc[\"Stock Adjustments\", :] = supply_corrections.sum(axis = 0) #Column correction\n",
    "    supply_table.loc[:, \"Stock Adjustments\"] = supply_corrections.sum(axis = 1) #Row correction\n",
    "    supply_table.loc[\"Stock Adjustments\", \"Stock Adjustments\"] = -supply_corrections.values.sum()\n",
    "\n",
    "# Correct negative values in supply table\n",
    "if(use_table < 0).any().any():\n",
    "    original_use_table = use_table.copy()\n",
    "    use_corrections = use_table.copy()\n",
    "    use_table[use_table < 0] = 0 #replace negatives with 0\n",
    "    use_corrections = (use_corrections - use_table) #store corrections\n",
    "    \n",
    "    # Add discrepancy row & column in the use table\n",
    "    use_table.loc[\"Stock Adjustments\", :] = use_corrections.sum(axis=0)  # Column correction\n",
    "    use_table.loc[:, \"Stock Adjustments\"] = use_corrections.sum(axis=1)  # Row correction\n",
    "    use_table.loc[\"Stock Adjustments\", \"Stock Adjustments\"] = -use_corrections.values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8acef",
   "metadata": {},
   "source": [
    "## 3. Regional Data\n",
    "The regional indicators used to map national SUTs to NUTS-2 level are loaded and cleaned.\n",
    "### 3.1 Set up Regional Accounts and SBS Data\n",
    "#### 3.1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6911ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\dzsve\\AppData\\Local\\Temp\\ipykernel_29024\\3009678733.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\":\", np.nan)\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\dzsve\\AppData\\Local\\Temp\\ipykernel_29024\\3009678733.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\":\", np.nan)\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\dzsve\\AppData\\Local\\Temp\\ipykernel_29024\\3009678733.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\":\", np.nan)\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\dzsve\\AppData\\Local\\Temp\\ipykernel_29024\\3009678733.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\":\", np.nan)\n",
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\dzsve\\AppData\\Local\\Temp\\ipykernel_29024\\3009678733.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\":\", np.nan)\n"
     ]
    }
   ],
   "source": [
    "rows_to_drop = [\"GEO (Labels)\", \"GEO (Codes)\", \"NACE_R2 (Labels)\", \"Extra-Regio NUTS 2\", \"ITM_NEWA (Labels)\", \"Special value\", \":\", \"Observation flags:\", \"p\", \"e\", \"u\", \n",
    "                \"b\", \"m\", \"d\", \"be\", \"de\", \"bd\", \"confidential\", \"estimated\", \"not available\", \"low reliability\", \"break in time series\"]\n",
    "\n",
    "def load_excel(file_name, sheet=\"Sheet 1\", skip_rows=8, index_col=0, drop_rows=rows_to_drop):\n",
    "    \"\"\"Loads and cleans an Excel file by dropping unwanted columns and empty rows.\"\"\"\n",
    "    df = pd.read_excel(os.path.join(data_dir, file_name), sheet_name=sheet, skiprows=skip_rows, index_col=index_col)\n",
    "    df = df[~df.index.isin(drop_rows)] # Removes specific rows by index \n",
    "    df = df.dropna(how='all')\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"Unnamed\")))]\n",
    "    df = df.replace(\":\", np.nan)\n",
    "    return df\n",
    "\n",
    "# Load regional data\n",
    "GVA_2022 = load_excel(\"nama_10r_3gva__GVA by region industry.xlsx\")\n",
    "CoE_2022 = load_excel(\"nama_10r_2coe__COE by region industry.xlsx\")\n",
    "Employment_2022 = load_excel(\"nama_10r_3empers__Employed persons by region industry.xlsx\", skip_rows=9)\n",
    "GDPpc_2022 = load_excel(\"nama_10r_3gdp__GDP per inhabitant by region.xlsx\", skip_rows=7)\n",
    "HoursWorked_2022 = load_excel(\"nama_10r_2emhrw__Hours worked by region industry.xlsx\", skip_rows=9)\n",
    "PrimaryIncome_2022 = load_excel(\"nama_10r_2hhinc__Balance primary incomes by region.xlsx\", skip_rows=9)\n",
    "Agriculture_2022 = load_excel(\"agr_r_accts__Agri regional accounts.xlsx\", skip_rows=9)\n",
    "\n",
    "\n",
    "# Load Structural Business Statistics (SBS) datasets\n",
    "SBSEmployment_2022 = load_excel(\"sbs_r_nuts2021__Persons employed by region industry.xlsx\", index_col = 1)\n",
    "SBSFirms_2022 = load_excel(\"sbs_r_nuts2021__Firms by region industry.xlsx\", index_col = 1)\n",
    "SBSWages_2022 = load_excel(\"sbs_r_nuts2021__Wages by region industry.xlsx\", index_col = 1)\n",
    "\n",
    "SBSEmployment_2022.drop(SBSEmployment_2022.tail(3).index,inplace=True)\n",
    "SBSFirms_2022.drop(SBSFirms_2022.tail(3).index,inplace=True)\n",
    "SBSWages_2022.drop(SBSWages_2022.tail(3).index,inplace=True)\n",
    "\n",
    "SBSEmployment_2022 = SBSEmployment_2022.iloc[:, 1:]\n",
    "SBSFirms_2022 = SBSFirms_2022.iloc[:, 1:]\n",
    "SBSWages_2022 = SBSWages_2022.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753afea",
   "metadata": {},
   "source": [
    "#### 3.1.2 Standardise NUTS Codes as Row Names across all Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2cc6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dzsve\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "# Load NUTS-2 region mapping from an SBS file (e.g. persons employed)\n",
    "NUTS_mapping = load_excel(\"sbs_r_nuts2021__Persons employed by region industry.xlsx\", index_col = False)\n",
    "NUTS_mapping = NUTS_mapping.iloc[2:289, :2]\n",
    "NUTS_mapping.columns = [\"NUTS_Code\", \"Region_Name\"]\n",
    "\n",
    "#remove rows that have have \"Extra-Regio NUTS 2\" as value in second column\n",
    "NUTS_mapping = NUTS_mapping.loc[~NUTS_mapping[\"Region_Name\"].str.contains(\"Extra-Regio NUTS 2\")]\n",
    "\n",
    "# Remove extra whitespace, standardise\n",
    "NUTS_mapping[\"NUTS_Code\"] = NUTS_mapping[\"NUTS_Code\"].str.strip()\n",
    "NUTS_mapping[\"Region_Name\"] = NUTS_mapping[\"Region_Name\"].str.strip()\n",
    "NUTS_mapping[\"Country_Code\"] = NUTS_mapping[\"NUTS_Code\"].str[:2]\n",
    "\n",
    "# Remove \"00\" from NUTS-2 codes\n",
    "NUTS_mapping[\"NUTS_Code\"] = NUTS_mapping[\"NUTS_Code\"].str.replace(\"00\", \"\", regex=False)\n",
    "\n",
    "# Remove duplicates\n",
    "NUTS_mapping = NUTS_mapping[~NUTS_mapping[\"NUTS_Code\"].duplicated(keep='first')]\n",
    "\n",
    "# Build a dictionary for NUTS-2 to NUTS-0 mapping\n",
    "regionname_to_nuts = NUTS_mapping.set_index(\"Region_Name\")[\"NUTS_Code\"].to_dict()\n",
    "\n",
    "#load official 2024 nuts codes\n",
    "NUTS2024 = pd.read_excel(os.path.join(data_dir,\"NUTS2021-NUTS2024.xlsx\"), sheet_name=\"NUTS2024\")\n",
    "NUTS2024 = NUTS2024.iloc[:, 0:4]\n",
    "NUTS2024 = NUTS2024[NUTS2024.iloc[:, 3] == 2]\n",
    "NUTS2024 = NUTS2024.iloc[:, 0:3]\n",
    "NUTS2024 = NUTS2024[~NUTS2024.iloc[:, 2].str.contains('Extra-Regio NUTS 2')]\n",
    "NUTS2024.iloc[:, 1] = NUTS2024.iloc[:, 1].astype(str).str.replace(\"00$\", \"\", regex=True)\n",
    "\n",
    "\n",
    "def convert_index_to_nuts(df, mapping_dict):\n",
    "    \"\"\"\n",
    "    Convert index from region names to NUTS codes using a dictionary and handle special region mappings.\n",
    "    If no exact match for region name in mapping_dict, keep original index value.\n",
    "    Handles special cases for NL31, NL33, PT16, PT17, PT18 regions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # First convert index using mapping dictionary\n",
    "    df.index = [mapping_dict.get(idx, idx) for idx in df.index]\n",
    "    \n",
    "    # Define special region mappings\n",
    "    special_mappings = {\n",
    "        'NL31': 'NL35',\n",
    "        'NL33': 'NL36', \n",
    "        'PT16': 'PT19',\n",
    "        'PT17': 'PT1A',\n",
    "        'PT18': 'PT1C'\n",
    "    }\n",
    "    \n",
    "    # Process each special region\n",
    "    for old_code, new_code in special_mappings.items():\n",
    "        if old_code in df.index:\n",
    "            # Check if row has any non-null values\n",
    "            if df.loc[old_code].notna().any():\n",
    "                # If new code doesn't exist, create it\n",
    "                if new_code not in df.index:\n",
    "                    df.loc[new_code] = df.loc[old_code]\n",
    "                # If new code exists but has all null values, fill with old code values\n",
    "                elif df.loc[new_code].isna().all():\n",
    "                    df.loc[new_code] = df.loc[old_code]\n",
    "                # If new code exists and has values, do nothing\n",
    "            # Remove old code row\n",
    "            df = df.drop(old_code)\n",
    "        elif new_code not in df.index:\n",
    "            # Create new code row if neither old nor new code exists\n",
    "            df.loc[new_code] = pd.Series(index=df.columns, dtype=float)\n",
    "\n",
    "    # Keep only rows where index is in either column 1 or column 2 of NUTS2024\n",
    "    df = df[df.index.isin(NUTS2024.loc[:, \"Country code\"]) | df.index.isin(NUTS2024.loc[:, \"NUTS Code\"])]\n",
    "    # Remove duplicate indices keeping first occurrence\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Create rows for any missing NUTS codes\n",
    "    missing_nuts = set(NUTS2024.loc[:, \"NUTS Code\"]) - set(df.index)\n",
    "    for nuts_code in missing_nuts:\n",
    "        df.loc[nuts_code] = pd.Series(index=df.columns, dtype=float)\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788b37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store all dataframes in a dictionary\n",
    "dfs = {\n",
    "    \"GVA_2022\": GVA_2022,\n",
    "    \"CoE_2022\": CoE_2022,\n",
    "    \"Employment_2022\": Employment_2022,\n",
    "    \"GDPpc_2022\": GDPpc_2022,\n",
    "    \"HoursWorked_2022\": HoursWorked_2022,\n",
    "    \"PrimaryIncome_2022\": PrimaryIncome_2022,\n",
    "    \"Agriculture_2022\": Agriculture_2022,\n",
    "    \"SBSEmployment_2022\": SBSEmployment_2022,\n",
    "    \"SBSFirms_2022\": SBSFirms_2022,\n",
    "    \"SBSWages_2022\": SBSWages_2022\n",
    "}\n",
    "\n",
    "# Apply conversion and cleaning to each dataframe\n",
    "for name in dfs:\n",
    "    dfs[name] = convert_index_to_nuts(dfs[name], regionname_to_nuts)\n",
    "\n",
    "# Unpack the cleaned dataframes back into your working variables\n",
    "(\n",
    "    GVA_2022, CoE_2022, Employment_2022, GDPpc_2022, HoursWorked_2022,\n",
    "    PrimaryIncome_2022, Agriculture_2022,\n",
    "    SBSEmployment_2022, SBSFirms_2022, SBSWages_2022\n",
    ") = dfs.values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678d5f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find indexes in SBSEmployment_2022 but not in Others\n",
    "only_in_sbs_agr = SBSEmployment_2022.index.difference(Agriculture_2022.index)\n",
    "only_in_sbs_coe = SBSEmployment_2022.index.difference(CoE_2022.index)\n",
    "only_in_sbs_prim_inc = SBSEmployment_2022.index.difference(PrimaryIncome_2022.index)\n",
    "only_in_sbs_hours = SBSEmployment_2022.index.difference(HoursWorked_2022.index)\n",
    "\n",
    "print(only_in_sbs_agr)\n",
    "print(only_in_sbs_coe)\n",
    "print(only_in_sbs_hours)\n",
    "print(only_in_sbs_prim_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d83fa",
   "metadata": {},
   "source": [
    "### 3.2 Clean Dataframes & Impute Missing Data\n",
    "#### 3.2.1 Check for Missing Values in Regional Datasets\n",
    "Verify if any values are missing in the SBS and regional accounts datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be3323a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GVA_2022: No missing values.\n",
      "CoE_2022: 24 missing values.\n",
      "Employment_2022: No missing values.\n",
      "GDPpc_2022: No missing values.\n",
      "HoursWorked_2022: 372 missing values.\n",
      "PrimaryIncome_2022: 2 missing values.\n",
      "Agriculture_2022: 330 missing values.\n",
      "SBSEmployment_2022: 1691 missing values.\n",
      "SBSFirms_2022: 833 missing values.\n",
      "SBSWages_2022: 2425 missing values.\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in dataframes\n",
    "def check_missing(df, name):\n",
    "    missing = df.isnull().sum().sum()\n",
    "    print(f\"{name}: {'No missing values.' if missing == 0 else f'{missing} missing values.'}\")\n",
    "    return missing\n",
    "\n",
    "missing_summary = {\n",
    "    \"GVA_2022\": check_missing(GVA_2022, \"GVA_2022\"),\n",
    "    \"CoE_2022\": check_missing(CoE_2022, \"CoE_2022\"),   \n",
    "    \"Employment_2022\": check_missing(Employment_2022, \"Employment_2022\"),\n",
    "    \"GDPpc_2022\": check_missing(GDPpc_2022, \"GDPpc_2022\"),\n",
    "    \"HoursWorked_2022\": check_missing(HoursWorked_2022, \"HoursWorked_2022\"),\n",
    "    \"PrimaryIncome_2022\": check_missing(PrimaryIncome_2022, \"PrimaryIncome_2022\"),\n",
    "    \"Agriculture_2022\": check_missing(Agriculture_2022, \"Agriculture_2022\"),\n",
    "    \"SBSEmployment_2022\": check_missing(SBSEmployment_2022, \"SBSEmployment_2022\"),\n",
    "    \"SBSFirms_2022\": check_missing(SBSFirms_2022, \"SBSFirms_2022\"),\n",
    "    \"SBSWages_2022\": check_missing(SBSWages_2022, \"SBSWages_2022\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d86ee172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values in SBSEmployment_2022_2digit:\n",
      "['B', 'B05', 'B06', 'B07', 'B08', 'B09', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C26', 'C27', 'C29', 'C30', 'C31', 'D', 'D35', 'E36', 'E37', 'E38', 'E39', 'G45', 'G47', 'H50', 'H51', 'H52', 'H53', 'I', 'I55', 'J59', 'J60', 'J61', 'J62', 'J63', 'K64', 'K65', 'K66', 'M69', 'M70', 'M72', 'M74', 'N77', 'N78', 'N79', 'N80', 'Q', 'Q86', 'R', 'R90', 'R91', 'R92', 'R93']\n",
      "Total number of columns with missing values: 62\n",
      "Rows with missing values in SBSEmployment_2022_2digit:\n",
      "['CZ', 'EE', 'IE', 'FR', 'IT', 'CY', 'LT', 'LU', 'HU', 'MT', 'AT', 'PL', 'PT', 'SI', 'SK']\n",
      "\n",
      "Total number of rows with missing values: 15\n"
     ]
    }
   ],
   "source": [
    "# Filter SBSEmployment_2022 to only include rows with 2-digit indices\n",
    "SBSEmployment_2022_2digit = SBSEmployment_2022[SBSEmployment_2022.index.str.len() == 2]\n",
    "\n",
    "# Print column names with missing values in SBSEmployment_2022_2digit\n",
    "missing_columns = SBSEmployment_2022_2digit.columns[SBSEmployment_2022_2digit.isnull().any()].tolist()\n",
    "print(\"Columns with missing values in SBSEmployment_2022_2digit:\")\n",
    "print(missing_columns)\n",
    "print(f\"Total number of columns with missing values: {len(missing_columns)}\")\n",
    "\n",
    "\n",
    "# Print all rows that have missing values in SBSEmployment_2022_2digit\n",
    "missing_rows = SBSEmployment_2022_2digit[SBSEmployment_2022_2digit.isnull().any(axis=1)]\n",
    "print(\"Rows with missing values in SBSEmployment_2022_2digit:\")\n",
    "print(missing_rows.index.tolist())\n",
    "print(f\"\\nTotal number of rows with missing values: {len(missing_rows)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec478fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest regions to CZ (diff=200.00): ['ES11', 'ES13']\n",
      "Closest regions to EE (diff=0.00): ['ES41']\n",
      "Closest regions to IE (diff=8100.00): ['DK01']\n",
      "Closest regions to FR (diff=0.00): ['DEB1']\n",
      "Closest regions to IT (diff=100.00): ['FRI1', 'PL91']\n",
      "Closest regions to CY (diff=100.00): ['BE35']\n",
      "Closest regions to LT (diff=0.00): ['CZ02']\n",
      "Closest regions to LU (diff=5000.00): ['IE05']\n",
      "Closest regions to HU (diff=100.00): ['PL']\n",
      "Closest regions to MT (diff=0.00): ['DEG0', 'FRJ2']\n",
      "Closest regions to AT (diff=100.00): ['DE23']\n",
      "Closest regions to PL (diff=100.00): ['EL65', 'HU']\n",
      "Closest regions to PT (diff=500.00): ['CZ02', 'LT']\n",
      "Closest regions to SI (diff=0.00): ['EL30']\n",
      "Closest regions to SK (diff=100.00): ['PT20']\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe as matrix of GDP per capita differences between all regions\n",
    "gdp_pc_matrix = pd.DataFrame(index=GDPpc_2022.index, columns=GDPpc_2022.index)\n",
    "\n",
    "for i in GDPpc_2022.index:\n",
    "    for j in GDPpc_2022.index:\n",
    "        gdp_pc_matrix.loc[i, j] = abs(GDPpc_2022.loc[i].iloc[0] - GDPpc_2022.loc[j].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef267b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in missing_rows.index:\n",
    "    # Get all regions except the country itself\n",
    "    other_regions = gdp_pc_matrix.loc[country].drop(country)\n",
    "    \n",
    "    # Find the minimum difference\n",
    "    min_diff = other_regions.min()\n",
    "    \n",
    "    # Find all regions with the minimum difference\n",
    "    closest_regions = other_regions[other_regions == min_diff].index.tolist()\n",
    "    \n",
    "    print(f\"Closest regions to {country} (diff={min_diff:.2f}): {closest_regions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280e9d2",
   "metadata": {},
   "source": [
    "### 3.2.2 Compute Missing Data in SBS Dataframes\n",
    "\n",
    "The imputation strategy uses GDP per capita similarity to match regions for filling missing data, with a preference for regions within the same country (25% weight reduction for cross-country matches). This approach ensures that missing values are imputed using data from economically similar regions, while still allowing cross-country matches when necessary. The imputed values are then scaled to ensure that the sum of regional values matches the national totals, while preserving all original data.\n",
    "\n",
    "Helper functions are used to apply this imputation strategy to all relevant datasets. The imputation strategy is described in detail below:\n",
    "\n",
    "**SBS Data Imputation Strategy**\n",
    "1. Similarity Calculation\n",
    "    - GDP Per Capita Similarity\n",
    "        - For each pair of regions:\n",
    "            - Calculate absolute difference in GDP per capita\n",
    "            - Convert to similarity score using 1/(1 + difference)\n",
    "            - Apply country preference factor:\n",
    "                - 4x higher weight for regions in same country\n",
    "                - 0.25x weight for regions in different countries\n",
    "        - Function: `prepare_similarity_matrix(gdp_pc, sbs_data)`\n",
    "\n",
    "2. Country-Level Imputation (2-digit codes) <br>\n",
    "    For each country and industry with missing data:\n",
    "    1. Find similar regions:\n",
    "      - Look at all regions with data for this industry\n",
    "        - Use similarity matrix to find most similar regions\n",
    "        - Calculate weights based on similarity scores\n",
    "    2. Calculate relative structure:\n",
    "        - For the country:\n",
    "            - Get available industries\n",
    "            - Calculate proportions of each industry\n",
    "        - For similar regions:\n",
    "            - Calculate their industry proportions\n",
    "            - Take weighted average based on similarity\n",
    "    - Function: `calculate_relative_structure(df, region, available_industries)` <br>\n",
    "\n",
    "    3. Impute missing value:\n",
    "        - If country has other industries:\n",
    "            - Use relative structure to estimate proportion\n",
    "            - Scale by total of available industries\n",
    "        - If no other industries:\n",
    "            - Use weighted average of similar regions' values\n",
    "    - Function: `impute_missing_sbs_data(sbs_data, similarity_matrix)`\n",
    "\n",
    "3. Allocation Calculation <br>\n",
    "    For each country and industry:\n",
    "    1. Get country total:\n",
    "        - Use country-level value (original or imputed)\n",
    "    2. Calculate regional sum:\n",
    "        - Sum all regional values (4-digit codes)\n",
    "    3. Calculate allocation:\n",
    "        - Allocation = Country Total - Regional Sum\n",
    "        - Positive: need to distribute more to regions\n",
    "        - Negative: regions sum to more than country total\n",
    "    - Function: `calculate_missing_allocation(df, country, industry)`\n",
    "\n",
    "4. Regional Imputation (4-digit codes) <br>\n",
    "    For each country and industry with missing regional data:\n",
    "    1. Find missing regions:\n",
    "        - Identify all regional rows with missing values\n",
    "        - Find all regions with available data\n",
    "    2. For each missing region:\n",
    "        - Calculate similarity:\n",
    "            - Find similar regions using similarity matrix\n",
    "            - Calculate weights based on similarity scores\n",
    "        - Calculate relative structure:\n",
    "            - For the region:\n",
    "                - Get available industries\n",
    "                - Calculate proportions of each industry\n",
    "            - For similar regions:\n",
    "                - Calculate their industry proportions\n",
    "                - Take weighted average based on similarity\n",
    "        - Function: `calculate_relative_structure(df, region, available_industries)` <br>\n",
    "\n",
    "        - Impute value:\n",
    "            - If region has other industries:\n",
    "                - Use relative structure to estimate proportion\n",
    "                - Scale by allocation amount\n",
    "            - If no other industries:\n",
    "                - Use weighted average of similar regions' values\n",
    "        - Function: `impute_missing_sbs_data(sbs_data, similarity_matrix)`\n",
    "    3. Scale imputed values:\n",
    "        - If allocation is positive:\n",
    "            - Sum all imputed values\n",
    "            - Scale to match allocation amount\n",
    "        - If allocation is negative:\n",
    "            - Keep imputed values as is\n",
    "        - Function: `impute_missing_sbs_data(sbs_data, similarity_matrix)`\n",
    "\n",
    "5. Convergence Check\n",
    "    - Compare imputed values with previous iteration\n",
    "        - If changes are below tolerance threshold, stop\n",
    "        - Otherwise, repeat regional imputation\n",
    "    - Function: `impute_missing_sbs_data(sbs_data, similarity_matrix)`\n",
    "\n",
    "6. Process All Datasets\n",
    "    - Apply the imputation process to all SBS datasets\n",
    "    - Verify results against country totals\n",
    "    - Function: `process_sbs_datasets(sbs_data, gdp_pc)`\n",
    "\n",
    "    <br>\n",
    "\n",
    "**Key Features**\n",
    "1. Hierarchical Approach\n",
    "    - Country-level data imputed first\n",
    "    - Regional data imputed while respecting country totals\n",
    "2. Relative Structure Preservation\n",
    "    - Uses industry proportions to guide imputation\n",
    "    - Maintains economic structure of regions\n",
    "3. Similarity Matching\n",
    "    - Uses GDP per capita similarity\n",
    "    - Gives preference to regions from same country\n",
    "4. Consistency Checks\n",
    "    - Ensures regional sums match country totals\n",
    "    - Handles both positive and negative allocations\n",
    "5. Iterative Refinement\n",
    "    - Multiple iterations to improve imputation\n",
    "    - Convergence check to ensure stability\n",
    "\n",
    "\n",
    "This strategy ensures that:\n",
    "- Country-level data is imputed first using similar regions\n",
    "- Regional data is imputed while maintaining consistency with country totals\n",
    "- The economic structure of regions is preserved\n",
    "- The imputation is based on GDP per capita similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1397dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_similarity_matrix(\n",
    "    gdp_pc: pd.DataFrame,\n",
    "    sbs_data: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a similarity matrix based on GDP per capita differences between regions.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Calculate GDP-based similarity using inverse of differences\n",
    "    2. Apply country preference factor (4x higher for same country)\n",
    "    \n",
    "    Args:\n",
    "        gdp_pc (pd.DataFrame): GDP per capita data by region\n",
    "        sbs_data (pd.DataFrame): SBS data with missing values (not used in this version)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: GDP-based similarity matrix\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing similarity matrix...\")\n",
    "    \n",
    "    # Initialize matrix\n",
    "    similarity_matrix = pd.DataFrame(index=gdp_pc.index, columns=gdp_pc.index)\n",
    "    \n",
    "    # Calculate GDP-based similarity\n",
    "    print(\"Calculating GDP-based similarities...\")\n",
    "    for r1 in gdp_pc.index:\n",
    "        for r2 in gdp_pc.index:\n",
    "            # Calculate absolute difference in GDP per capita\n",
    "            diff = abs(gdp_pc.loc[r1, 'GDPpc_2022'] - gdp_pc.loc[r2, 'GDPpc_2022'])\n",
    "            # Convert difference to similarity score\n",
    "            gdp_score = 1 / (1 + diff)\n",
    "            \n",
    "            # Apply country preference factor\n",
    "            if r1[:2] == r2[:2]:  # Same country\n",
    "                similarity_matrix.loc[r1, r2] = gdp_score\n",
    "            else:  # Different country\n",
    "                similarity_matrix.loc[r1, r2] = gdp_score * 0.25\n",
    "    \n",
    "    print(\"Similarity matrix preparation complete\")\n",
    "    return similarity_matrix\n",
    "\n",
    "def calculate_missing_allocation(\n",
    "    df: pd.DataFrame,\n",
    "    country: str,\n",
    "    industry: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the amount to be allocated for a specific industry in a country.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Get country-level total (2-digit code)\n",
    "    2. Sum up all regional data (4-digit codes)\n",
    "    3. Calculate difference (positive means we need to allocate more)\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Data with missing values\n",
    "        country (str): Country code\n",
    "        industry (str): Industry code\n",
    "        \n",
    "    Returns:\n",
    "        float: Amount to be allocated for this industry\n",
    "    \"\"\"\n",
    "    # Get country-level data and regional sum\n",
    "    country_total = df.loc[country, industry]\n",
    "    regional_sum = df[df.index.str.startswith(country) & \n",
    "                     (df.index.str.len() == 4)][industry].sum()\n",
    "    \n",
    "    # Calculate allocation needed\n",
    "    allocation = country_total - regional_sum\n",
    "    return allocation\n",
    "\n",
    "def impute_missing_sbs_data(\n",
    "    sbs_data: pd.DataFrame,\n",
    "    similarity_matrix: pd.DataFrame,\n",
    "    gdp_pc: pd.DataFrame = None,\n",
    "    max_iterations: int = 100,\n",
    "    tolerance: float = 1e-6\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing SBS data using GDP per capita similarity.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Impute country-level data using similar regions\n",
    "    2. Calculate missing allocations for each country and industry\n",
    "    3. Impute regional data while respecting allocations\n",
    "    \n",
    "    Args:\n",
    "        sbs_data (pd.DataFrame): SBS data with missing values\n",
    "        similarity_matrix (pd.DataFrame): GDP-based similarity matrix\n",
    "        gdp_pc (pd.DataFrame, optional): GDP per capita data. If None, will use existing similarity matrix\n",
    "        max_iterations (int): Maximum number of iterations for convergence\n",
    "        tolerance (float): Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed SBS data\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting SBS data imputation...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    imputed_data = sbs_data.copy()\n",
    "    \n",
    "    # Convert values with \":\" to NaN\n",
    "    imputed_data = imputed_data.replace(\":\", np.nan)\n",
    "    \n",
    "    # If GDP per capita data is provided, recalculate similarity matrix\n",
    "    if gdp_pc is not None:\n",
    "        print(\"Recalculating similarity matrix with new GDP data...\")\n",
    "        similarity_matrix = prepare_similarity_matrix(gdp_pc, sbs_data)\n",
    "    \n",
    "    # Step 1: Impute country-level data\n",
    "    print(\"\\nStep 1: Imputing country-level data...\")\n",
    "    country_rows = imputed_data[imputed_data.index.str.len() == 2]\n",
    "    for country in country_rows.index:\n",
    "        for industry in country_rows.columns:\n",
    "            if pd.isna(country_rows.loc[country, industry]):\n",
    "                # Get similar regions with data for this industry\n",
    "                available_regions = imputed_data.index[~imputed_data[industry].isna()]\n",
    "                similar_regions = similarity_matrix.loc[country, available_regions]\n",
    "                weights = similar_regions / similar_regions.sum()\n",
    "                \n",
    "                # Use weighted average of available values\n",
    "                imputed_value = (imputed_data.loc[available_regions, industry] * weights).sum()\n",
    "                imputed_data.loc[country, industry] = imputed_value\n",
    "    \n",
    "    # Step 2: Calculate missing allocations for each country and industry\n",
    "    print(\"\\nStep 2: Calculating missing allocations...\")\n",
    "    missing_allocations = {}\n",
    "    for country in imputed_data.index.str[:2].unique():\n",
    "        missing_allocations[country] = {}\n",
    "        for industry in imputed_data.columns:\n",
    "            missing_allocations[country][industry] = calculate_missing_allocation(\n",
    "                imputed_data, country, industry\n",
    "            )\n",
    "    \n",
    "    # Step 3: Impute regional data\n",
    "    for iteration in range(max_iterations):\n",
    "        # Store previous state for convergence check\n",
    "        previous_data = imputed_data.copy()\n",
    "        \n",
    "        # Process each country separately\n",
    "        for country in imputed_data.index.str[:2].unique():\n",
    "            # Get data for current country\n",
    "            country_mask = imputed_data.index.str.startswith(country)\n",
    "            country_data = imputed_data[country_mask]\n",
    "            \n",
    "            # Process each industry\n",
    "            for industry in country_data.columns:\n",
    "                missing_mask = country_data[industry].isna()\n",
    "                if missing_mask.any():\n",
    "                    # Identify regions with missing and available data\n",
    "                    missing_regions = country_data.index[missing_mask]\n",
    "                    available_regions = imputed_data.index[~imputed_data[industry].isna()]\n",
    "                    \n",
    "                    # Get the amount to be allocated for this industry\n",
    "                    allocation = missing_allocations[country][industry]\n",
    "                    \n",
    "                    # Impute each missing value\n",
    "                    for region in missing_regions:\n",
    "                        # Get similarity scores and calculate weights\n",
    "                        similarities = similarity_matrix.loc[region, available_regions]\n",
    "                        weights = similarities / similarities.sum()\n",
    "                        \n",
    "                        # Use weighted average of available values\n",
    "                        imputed_value = (imputed_data.loc[available_regions, industry] * weights).sum()\n",
    "                        imputed_data.loc[region, industry] = imputed_value\n",
    "                    \n",
    "                    # Scale imputed values to match allocation if positive\n",
    "                    if len(missing_regions) > 0 and allocation > 0:\n",
    "                        imputed_total = imputed_data.loc[missing_regions, industry].sum()\n",
    "                        if imputed_total > 0:  # Avoid division by zero\n",
    "                            scaling_factor = allocation / imputed_total\n",
    "                            imputed_data.loc[missing_regions, industry] *= scaling_factor\n",
    "        \n",
    "        # Check if the solution has converged\n",
    "        if (abs(imputed_data - previous_data) < tolerance).all().all():\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "def process_sbs_datasets(\n",
    "    sbs_data: Dict[str, pd.DataFrame],\n",
    "    gdp_pc: pd.DataFrame\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process all SBS datasets to impute missing values.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Create similarity matrix for each dataset\n",
    "    2. Impute missing values using GDP per capita similarity\n",
    "    3. Verify results against country totals\n",
    "    \n",
    "    Args:\n",
    "        sbs_data (Dict[str, pd.DataFrame]): Dictionary of SBS datasets\n",
    "        gdp_pc (pd.DataFrame): GDP per capita data\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary of imputed SBS datasets\n",
    "    \"\"\"\n",
    "    # Process each SBS dataset\n",
    "    for dataset_name in ['SBSEmployment_2022', 'SBSFirms_2022', 'SBSWages_2022']:\n",
    "        print(f\"\\nProcessing {dataset_name}...\")\n",
    "        \n",
    "        # Create similarity matrix for current dataset\n",
    "        similarity_matrix = prepare_similarity_matrix(gdp_pc, sbs_data[dataset_name])\n",
    "        \n",
    "        # Perform imputation\n",
    "        imputed_data = impute_missing_sbs_data(\n",
    "            sbs_data[dataset_name],\n",
    "            similarity_matrix\n",
    "        )\n",
    "        \n",
    "        # Update dataset with imputed values\n",
    "        sbs_data[dataset_name] = imputed_data\n",
    "    \n",
    "        # Report results\n",
    "        missing_before = sbs_data[dataset_name].isna().sum().sum()\n",
    "        missing_after = imputed_data.isna().sum().sum()\n",
    "        print(f\"Missing values before: {missing_before}\")\n",
    "        print(f\"Missing values after: {missing_after}\")\n",
    "        \n",
    "        # Verify accuracy against country totals\n",
    "        for country in imputed_data.index.str[:2].unique():\n",
    "            country_total = imputed_data.loc[country]\n",
    "            regional_sum = imputed_data[imputed_data.index.str.startswith(country) & \n",
    "                                     (imputed_data.index.str.len() == 4)].sum()\n",
    "            max_diff = abs(country_total - regional_sum).max()\n",
    "            print(f\"{country} - Maximum difference from country total: {max_diff:.2f}\")\n",
    "    \n",
    "    return sbs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994dcef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SBSEmployment_2022...\n",
      "\n",
      "Preparing similarity matrix...\n",
      "Calculating GDP-based similarities...\n",
      "Similarity matrix preparation complete\n",
      "\n",
      "Starting SBS data imputation...\n",
      "\n",
      "Step 1: Imputing country-level data...\n",
      "\n",
      "Step 2: Calculating missing allocations...\n",
      "Converged after 2 iterations\n",
      "Missing values before: 0\n",
      "Missing values after: 0\n",
      "BE - Maximum difference from country total: 7873.00\n",
      "BG - Maximum difference from country total: 0.00\n",
      "CZ - Maximum difference from country total: 2.00\n",
      "DK - Maximum difference from country total: 0.00\n",
      "DE - Maximum difference from country total: 4.00\n",
      "EE - Maximum difference from country total: 111411.00\n",
      "IE - Maximum difference from country total: 31144.13\n",
      "EL - Maximum difference from country total: 32.00\n",
      "ES - Maximum difference from country total: 3.00\n",
      "FR - Maximum difference from country total: 985.07\n",
      "HR - Maximum difference from country total: 0.00\n",
      "IT - Maximum difference from country total: 134324.40\n",
      "CY - Maximum difference from country total: 75841.00\n",
      "LV - Maximum difference from country total: 145898.00\n",
      "LT - Maximum difference from country total: 0.00\n",
      "LU - Maximum difference from country total: 55015.00\n",
      "HU - Maximum difference from country total: 0.00\n",
      "MT - Maximum difference from country total: 41270.00\n",
      "NL - Maximum difference from country total: 2.00\n",
      "AT - Maximum difference from country total: 0.00\n",
      "PL - Maximum difference from country total: 14408.37\n",
      "PT - Maximum difference from country total: 384170.69\n",
      "RO - Maximum difference from country total: 0.00\n",
      "SI - Maximum difference from country total: 1.00\n",
      "SK - Maximum difference from country total: 4.00\n",
      "FI - Maximum difference from country total: 2.00\n",
      "SE - Maximum difference from country total: 303.24\n",
      "\n",
      "Processing SBSFirms_2022...\n",
      "\n",
      "Preparing similarity matrix...\n",
      "Calculating GDP-based similarities...\n",
      "Similarity matrix preparation complete\n",
      "\n",
      "Starting SBS data imputation...\n",
      "\n",
      "Step 1: Imputing country-level data...\n",
      "\n",
      "Step 2: Calculating missing allocations...\n",
      "Converged after 2 iterations\n",
      "Missing values before: 0\n",
      "Missing values after: 0\n",
      "BE - Maximum difference from country total: 2743.00\n",
      "BG - Maximum difference from country total: 0.00\n",
      "CZ - Maximum difference from country total: 5.00\n",
      "DK - Maximum difference from country total: 0.00\n",
      "DE - Maximum difference from country total: 0.00\n",
      "EE - Maximum difference from country total: 30175.00\n",
      "IE - Maximum difference from country total: 4656.53\n",
      "EL - Maximum difference from country total: 15.00\n",
      "ES - Maximum difference from country total: 4.00\n",
      "FR - Maximum difference from country total: 54.21\n",
      "HR - Maximum difference from country total: 0.00\n",
      "IT - Maximum difference from country total: 18222.78\n",
      "CY - Maximum difference from country total: 19076.00\n",
      "LV - Maximum difference from country total: 35405.00\n",
      "LT - Maximum difference from country total: 0.00\n",
      "LU - Maximum difference from country total: 10996.00\n",
      "HU - Maximum difference from country total: 0.00\n",
      "MT - Maximum difference from country total: 9823.00\n",
      "NL - Maximum difference from country total: 1403.00\n",
      "AT - Maximum difference from country total: 0.00\n",
      "PL - Maximum difference from country total: 0.00\n",
      "PT - Maximum difference from country total: 138.00\n",
      "RO - Maximum difference from country total: 0.00\n",
      "SI - Maximum difference from country total: 0.00\n",
      "SK - Maximum difference from country total: 4.00\n",
      "FI - Maximum difference from country total: 1.00\n",
      "SE - Maximum difference from country total: 8.75\n",
      "\n",
      "Processing SBSWages_2022...\n",
      "\n",
      "Preparing similarity matrix...\n",
      "Calculating GDP-based similarities...\n",
      "Similarity matrix preparation complete\n",
      "\n",
      "Starting SBS data imputation...\n",
      "\n",
      "Step 1: Imputing country-level data...\n",
      "\n",
      "Step 2: Calculating missing allocations...\n",
      "Converged after 2 iterations\n",
      "Missing values before: 0\n",
      "Missing values after: 0\n",
      "BE - Maximum difference from country total: 293.84\n",
      "BG - Maximum difference from country total: 0.02\n",
      "CZ - Maximum difference from country total: 0.02\n",
      "DK - Maximum difference from country total: 0.01\n",
      "DE - Maximum difference from country total: 0.05\n",
      "EE - Maximum difference from country total: 2060.00\n",
      "IE - Maximum difference from country total: 2860.71\n",
      "EL - Maximum difference from country total: 0.19\n",
      "ES - Maximum difference from country total: 118.45\n",
      "FR - Maximum difference from country total: 72.62\n",
      "HR - Maximum difference from country total: 0.01\n",
      "IT - Maximum difference from country total: 3900.15\n",
      "CY - Maximum difference from country total: 1326.36\n",
      "LV - Maximum difference from country total: 1785.74\n",
      "LT - Maximum difference from country total: 0.01\n",
      "LU - Maximum difference from country total: 5613.56\n",
      "HU - Maximum difference from country total: 0.03\n",
      "MT - Maximum difference from country total: 704.49\n",
      "NL - Maximum difference from country total: 1590.15\n",
      "AT - Maximum difference from country total: 0.03\n",
      "PL - Maximum difference from country total: 545.23\n",
      "PT - Maximum difference from country total: 4277.15\n",
      "RO - Maximum difference from country total: 122.77\n",
      "SI - Maximum difference from country total: 69.88\n",
      "SK - Maximum difference from country total: 12.03\n",
      "FI - Maximum difference from country total: 0.02\n",
      "SE - Maximum difference from country total: 2181.83\n",
      "Created imputed dataframes:\n",
      "- Imputed_SBSEmployment_2022\n",
      "- Imputed_SBSFirms_2022\n",
      "- Imputed_SBSWages_2022\n"
     ]
    }
   ],
   "source": [
    "sbs_data = {\n",
    "    'SBSEmployment_2022': SBSEmployment_2022,\n",
    "    'SBSFirms_2022': SBSFirms_2022, \n",
    "    'SBSWages_2022': SBSWages_2022\n",
    "}\n",
    "GDPpc_2022 = GDPpc_2022.rename(columns={'2022': 'GDPpc_2022'})\n",
    "imputed_sbs_data = process_sbs_datasets(sbs_data, GDPpc_2022)\n",
    "\n",
    "# Create new dataframes with imputed data\n",
    "Imputed_SBSEmployment_2022 = imputed_sbs_data['SBSEmployment_2022']\n",
    "Imputed_SBSFirms_2022 = imputed_sbs_data['SBSFirms_2022']\n",
    "Imputed_SBSWages_2022 = imputed_sbs_data['SBSWages_2022']\n",
    "\n",
    "print(\"Created imputed dataframes:\")\n",
    "print(\"- Imputed_SBSEmployment_2022\")\n",
    "print(\"- Imputed_SBSFirms_2022\")\n",
    "print(\"- Imputed_SBSWages_2022\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
